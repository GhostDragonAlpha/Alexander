# Alexander Network Metrics Tracking Summary

## Overview

This document details all network metrics now being tracked in real-time within Alexander's networking systems.

---

## 1. Latency Metrics

### Average Latency (Round-Trip Time)
- **Type**: Float (milliseconds)
- **Source**: `UNetDriver` â†’ `APlayerState::ExactPing`
- **Update Frequency**: 1 Hz (updated every second)
- **Calculation**:
  - **Server**: Average of all client pings
  - **Client**: Own ping from PlayerState
- **Formula**: `TotalPing / ValidConnections`
- **Accessed via**: `NetworkReplicationManager->GetNetworkStatistics().AverageLatency`
- **Range**: 0-500ms typical, 500-1000ms poor, >1000ms critical

### Per-Client Latency
- **Type**: Float per client (milliseconds)
- **Source**: Individual `APlayerState::ExactPing` values
- **Update Frequency**: Real-time (engine managed)
- **Accessed via**: `PlayerController->PlayerState->ExactPing`

### Historical Ping Data
- **Type**: Array of floats
- **Buffer Size**: 60 samples (1 minute at 1Hz)
- **Purpose**: Trend analysis, lag spike detection
- **Accessed via**: `NetworkManager->PingHistory`

---

## 2. Packet Loss Metrics

### Packet Loss Percentage
- **Type**: Float (0.0 to 1.0)
- **Source**: `UNetDriver::OutPacketsLost / OutPackets`
- **Update Frequency**: Per frame
- **Smoothing**: Exponential moving average (Alpha = 0.1)
- **Formula**: `PacketLoss = 0.1 * Current + 0.9 * Previous`
- **Accessed via**: `NetworkManager->GetPacketLoss()`
- **Range**: 0-2% acceptable, 2-5% poor, >5% critical

### Raw Packet Counts
- **Type**: Int32
- **Tracked Metrics**:
  - `OutPackets`: Total packets sent
  - `OutPacketsLost`: Packets lost in transmission
  - `InPackets`: Total packets received
- **Source**: `UNetDriver`
- **Accessed via**: `NetDriver->OutPackets`, `NetDriver->OutPacketsLost`

---

## 3. Bandwidth Metrics

### Bytes Sent Per Second
- **Type**: Int32 (bytes)
- **Source**: Accumulated entity replication bandwidth
- **Calculation**: Sum of all entity replication data sent
- **Estimate**: 48 bytes per entity (Position + Rotation + Velocity + Metadata)
- **Update Frequency**: Per replication cycle (~30Hz)
- **Accessed via**: `NetworkReplicationManager->GetNetworkStatistics().BytesSent`
- **Typical Range**: 10-100 KB/s per client

### Bytes Received Per Second
- **Type**: Int32 (bytes)
- **Source**: `CurrentFrameBytesUsed` tracking
- **Update Frequency**: Per frame
- **Accessed via**: `NetworkReplicationManager->GetNetworkStatistics().BytesReceived`
- **Typical Range**: 10-100 KB/s

### Current Frame Bandwidth Usage
- **Type**: Int32 (bytes)
- **Source**: Per-frame accumulation
- **Reset**: Every frame
- **Purpose**: Real-time bandwidth throttling
- **Accessed via**: `NetworkReplicationManager->GetCurrentBandwidthUsage()`

### Bandwidth Budget
- **Type**: Float (0.0 to 1.0)
- **Calculation**: `(MaxBytesPerFrame - CurrentUsed) / MaxBytesPerFrame`
- **Purpose**: Adaptive quality of service
- **Accessed via**: `NetworkReplicationManager->GetReplicationBudget()`
- **Threshold**: <0.2 triggers bandwidth optimization

---

## 4. Compression Metrics

### Compression Ratio
- **Type**: Float (0.0 to 1.0)
- **Calculation**: `TotalBytesAfterCompression / TotalBytesBeforeCompression`
- **Algorithm**: Zlib with COMPRESS_BiasSpeed
- **Update Frequency**: Per replication cycle
- **Accessed via**: `ReplicationManager->GetReplicationStats().CompressionRatio`
- **Typical Range**: 0.3-0.7 (30-70% size reduction)

### Bytes Before Compression
- **Type**: Int32 (cumulative)
- **Source**: Sum of all uncompressed data
- **Reset**: On statistics reset
- **Accessed via**: `ReplicationManager->TotalBytesBeforeCompression` (private)

### Bytes After Compression
- **Type**: Int32 (cumulative)
- **Source**: Sum of all compressed data
- **Reset**: On statistics reset
- **Accessed via**: `ReplicationManager->TotalBytesAfterCompression` (private)

### Compression Savings
- **Type**: Int32 (bytes saved)
- **Calculation**: `BytesBefore - BytesAfter`
- **Purpose**: Measure compression effectiveness
- **Derived**: Not directly stored, calculated from ratio

---

## 5. Connection Metrics

### Active Connections
- **Type**: Int32 (count)
- **Source**: `UNetDriver::ClientConnections.Num()`
- **Update Frequency**: Real-time
- **Accessed via**: `NetworkReplicationManager->GetNetworkStatistics().ActiveConnections`
- **Range**: 0 to MaxPlayers

### Connection State
- **Type**: Enum (EAlexanderConnectionState)
- **Values**: Disconnected, Connecting, Connected, Loading, Ready, Error
- **Update Frequency**: On state change
- **Accessed via**: `NetworkManager->GetConnectionState()`

---

## 6. Replication Metrics

### Entities Replicated
- **Type**: Int32 (count)
- **Source**: Registered entity count
- **Update Frequency**: On entity register/unregister
- **Accessed via**: `NetworkReplicationManager->GetNetworkStatistics().EntitiesReplicated`

### Entities Replicated This Frame
- **Type**: Int32 (count)
- **Source**: Per-frame replication counter
- **Reset**: Every frame
- **Purpose**: Frame-by-frame replication monitoring
- **Accessed via**: `ReplicationManager->Stats.ActorsReplicatedThisFrame`

### Total Actors Replicated
- **Type**: Int32 (cumulative count)
- **Source**: Lifetime replication counter
- **Reset**: On statistics reset
- **Accessed via**: `ReplicationManager->Stats.TotalActorsReplicated`

### Average Replication Time
- **Type**: Float (milliseconds)
- **Calculation**: Rolling average of replication times
- **Purpose**: Performance monitoring
- **Accessed via**: `ReplicationManager->Stats.AverageReplicationTime`

### Packets Sent/Received
- **Type**: Int32 (count)
- **Source**: Replication packet counter
- **Update Frequency**: Per packet
- **Accessed via**:
  - `ReplicationManager->Stats.PacketsSent`
  - `ReplicationManager->Stats.PacketsReceived`

---

## 7. Prediction & Reconciliation Metrics

### Average Prediction Error
- **Type**: Float (centimeters)
- **Calculation**: Rolling average of position error magnitude
- **Formula**: `Distance(ClientState, ServerState)`
- **Update Frequency**: On server reconciliation
- **Accessed via**: `NetworkPhysics->GetAveragePredictionError()`
- **Acceptable Range**: 0-5cm excellent, 5-20cm acceptable, >20cm needs tuning

### Reconciliation Count
- **Type**: Int32 (count)
- **Source**: Cumulative reconciliation events
- **Reset**: On statistics reset
- **Purpose**: Monitor prediction accuracy
- **Accessed via**: `NetworkPhysics->GetReconciliationCount()`

### Input Buffer Size
- **Type**: Int32 (count)
- **Source**: Current size of input history buffer
- **Max**: 100 entries
- **Purpose**: Monitor input replay system health
- **Accessed via**: `NetworkPhysics->InputHistory[Actor].InputData.Num()`

---

## 8. Server Node Metrics

### Server Load Percentage
- **Type**: Float (0-100%)
- **Components**:
  - Player Load: 40% weight
  - CPU Usage: 30% weight
  - Memory Usage: 20% weight
  - Bandwidth Usage: 10% weight
- **Update Frequency**: 1 Hz
- **Accessed via**: `ServerNode->GetLoadPercentage()`

### Server Load Status
- **Type**: Enum (EServerLoadStatus)
- **Values**: Idle, Low, Medium, High, Overloaded, Critical
- **Thresholds**:
  - Idle: 0-20%
  - Low: 20-40%
  - Medium: 40-60%
  - High: 60-80%
  - Overloaded: 80-95%
  - Critical: 95-100%
- **Accessed via**: `ServerNode->LoadStatus`

### Players Per Server Node
- **Type**: Int32 (count)
- **Source**: Connected players count
- **Accessed via**: `ServerNode->CurrentPlayers`
- **Max**: `ServerNode->MaxPlayers`

### CPU Usage
- **Type**: Float (0-100%)
- **Source**: Simulated (in production, use platform CPU monitoring)
- **Update Frequency**: 1 Hz
- **Accessed via**: `ServerNode->CPUUsage`

### Memory Usage
- **Type**: Float (0-100%)
- **Source**: Simulated (in production, use FPlatformMemory)
- **Update Frequency**: 1 Hz
- **Accessed via**: `ServerNode->MemoryUsage`

### Bandwidth Usage Per Node
- **Type**: Float (KB/s)
- **Calculation**: `CurrentPlayers * RandRange(1-5)`
- **Update Frequency**: 1 Hz
- **Accessed via**: `ServerNode->BandwidthUsage`

### Average Node Ping
- **Type**: Float (milliseconds)
- **Source**: Average of all player pings on node
- **Accessed via**: `ServerNode->GetAveragePing()`

### Node Packet Loss
- **Type**: Float (percentage)
- **Calculation**: `LostPackets / SentPackets * 100`
- **Accessed via**: `ServerNode->GetPacketLoss()`

---

## 9. Server Mesh Zone Metrics

### Zone Player Count
- **Type**: Int32 (count)
- **Source**: Players within zone radius
- **Update Frequency**: On zone update
- **Accessed via**: `ServerMeshZone.PlayerCount`

### Zone Entity Count
- **Type**: Int32 (count)
- **Source**: Entities within zone
- **Accessed via**: `ServerMeshZone.EntityCount`

### Zone Server Load
- **Type**: Float (0-100%)
- **Source**: Handling server's load percentage
- **Accessed via**: `ServerMeshZone.ServerLoad`

### Zone Active Status
- **Type**: Bool
- **Purpose**: Zone availability for transfers
- **Accessed via**: `ServerMeshZone.bIsActive`

---

## Metrics Dashboard Integration

### Real-Time Display
All metrics can be polled and displayed in UMG widgets:

```cpp
// Network Overview
FAlexanderNetworkStatistics NetStats = NetworkReplicationManager->GetNetworkStatistics();
float Ping = NetworkManager->GetPing();
float PacketLoss = NetworkManager->GetPacketLoss();

// Replication Stats
FReplicationStats RepStats = ReplicationManager->GetReplicationStats();

// Physics Stats
float PredError = NetworkPhysics->GetAveragePredictionError();
int32 Reconciliations = NetworkPhysics->GetReconciliationCount();

// Server Stats
float ServerLoad = ServerNode->GetLoadPercentage();
```

### Logging
All metrics support UE_LOG for debugging:

```cpp
UE_LOG(LogTemp, Log, TEXT("Network Stats - Ping: %.2fms, Loss: %.2f%%, Sent: %dKB"),
    NetStats.AverageLatency,
    NetStats.PacketLoss * 100.0f,
    NetStats.BytesSent / 1024);
```

### CSV Export (Optional)
Consider adding CSV export for metric analysis:

```cpp
FString CSVLine = FString::Printf(TEXT("%.2f,%.4f,%d,%d,%.2f"),
    NetStats.AverageLatency,
    NetStats.PacketLoss,
    NetStats.BytesSent,
    NetStats.BytesReceived,
    RepStats.CompressionRatio);
```

---

## Performance Impact

### Memory Footprint
- NetworkReplicationManager Statistics: ~60 bytes
- ReplicationManager Statistics: ~40 bytes
- NetworkPhysics per actor: ~800 bytes (100 input history entries)
- ServerNode metrics: ~100 bytes
- Ping History (60 samples): ~240 bytes

**Total per client**: ~1.2 KB

### CPU Overhead
- Latency calculation: <0.01ms per frame
- Packet loss smoothing: <0.01ms per frame
- Compression: ~0.1-1ms per KB (depends on data)
- Input replay: ~0.1ms per reconciliation
- Statistics aggregation: <0.1ms per frame

**Total overhead**: <1% CPU usage typical

---

## Alerting Thresholds

### Warning Levels
```cpp
// Latency
const float PING_WARNING = 100.0f;  // ms
const float PING_CRITICAL = 200.0f; // ms

// Packet Loss
const float LOSS_WARNING = 0.02f;   // 2%
const float LOSS_CRITICAL = 0.05f;  // 5%

// Bandwidth
const float BW_WARNING = 80.0f;     // KB/s
const float BW_CRITICAL = 120.0f;   // KB/s

// Server Load
const float LOAD_WARNING = 70.0f;   // %
const float LOAD_CRITICAL = 85.0f;  // %

// Prediction Error
const float ERROR_WARNING = 10.0f;  // cm
const float ERROR_CRITICAL = 50.0f; // cm
```

---

## Summary

**Total Metrics Tracked**: 35+ individual metrics across 9 categories

**Update Frequencies**:
- Real-time (per frame): 13 metrics
- 1 Hz (per second): 8 metrics
- On event: 14 metrics

**Data Sources**:
- UNetDriver: 8 metrics
- Custom tracking: 27 metrics

**Performance**: <1% CPU overhead, ~1.2KB memory per client
